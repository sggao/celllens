{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9181,"status":"ok","timestamp":1686260313598,"user":{"displayName":"Bokai Zhu","userId":"09132585489018838978"},"user_tz":420},"id":"oefrh8CDnVYR","outputId":"c13ff236-b95f-457a-cf75-2a5f4ba0fe08"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting muse_sc\n","  Downloading muse_sc-0.0.8-py3-none-any.whl (8.9 kB)\n","Installing collected packages: muse_sc\n","Successfully installed muse_sc-0.0.8\n"]}],"source":["!pip install muse_sc"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18283,"status":"ok","timestamp":1686260331877,"user":{"displayName":"Bokai Zhu","userId":"09132585489018838978"},"user_tz":420},"id":"vXfbPte-sUhK","outputId":"d0872a2d-eaf1-47f2-e65d-54f98b2c280b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# loading some basic packages\n","from google.colab import drive\n","import os\n","import sys\n","#import torch\n","#os.environ['TORCH'] = torch.__version__\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15906,"status":"ok","timestamp":1686260347780,"user":{"displayName":"Bokai Zhu","userId":"09132585489018838978"},"user_tz":420},"id":"NrAls7dWulQi","outputId":"22d7dfce-5030-474c-e2f5-cde61fe26691"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PhenoGraph\n","  Downloading PhenoGraph-1.5.7-py3-none-any.whl (159 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting leidenalg>=0.8.2 (from PhenoGraph)\n","  Downloading leidenalg-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=18.0.1 in /usr/local/lib/python3.10/dist-packages (from PhenoGraph) (67.7.2)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from PhenoGraph) (1.22.4)\n","Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from PhenoGraph) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.10/dist-packages (from PhenoGraph) (1.2.2)\n","Requirement already satisfied: psutil>4 in /usr/local/lib/python3.10/dist-packages (from PhenoGraph) (5.9.5)\n","Collecting igraph<0.11,>=0.10.0 (from leidenalg>=0.8.2->PhenoGraph)\n","  Downloading igraph-0.10.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17->PhenoGraph) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17->PhenoGraph) (3.1.0)\n","Collecting texttable>=1.6.2 (from igraph<0.11,>=0.10.0->leidenalg>=0.8.2->PhenoGraph)\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable, igraph, leidenalg, PhenoGraph\n","Successfully installed PhenoGraph-1.5.7 igraph-0.10.4 leidenalg-0.9.1 texttable-1.6.7\n"]}],"source":["!pip install PhenoGraph"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5557,"status":"ok","timestamp":1686260353323,"user":{"displayName":"Bokai Zhu","userId":"09132585489018838978"},"user_tz":420},"id":"V9NH4ViMuP6L","outputId":"c40ce359-6bb3-445d-b64d-9e6f072d0dad"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}],"source":["import muse_sc as muse\n","import phenograph\n","from sklearn.decomposition import PCA\n","import numpy as np\n","from sklearn.metrics.cluster import adjusted_rand_score\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR')\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"hPWuOuJ62rq-"},"source":["We do batch for loop in this case"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278532,"status":"ok","timestamp":1686260631842,"user":{"displayName":"Bokai Zhu","userId":"09132585489018838978"},"user_tz":420},"id":"qyBW_LJ-2q9p","outputId":"8a277f10-70a3-4760-c68a-7472db908a57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finding 30 nearest neighbors using minkowski metric and 'auto' algorithm\n","Neighbors computed in 15.36022686958313 seconds\n","Jaccard graph constructed in 17.616031646728516 seconds\n","Wrote graph to binary file in 0.8761038780212402 seconds\n","Running Louvain modularity optimization\n","After 1 runs, maximum modularity is Q = 0.84321\n","After 6 runs, maximum modularity is Q = 0.845458\n","After 15 runs, maximum modularity is Q = 0.846621\n","After 16 runs, maximum modularity is Q = 0.847645\n","Louvain completed 36 runs in 96.366703748703 seconds\n","Sorting communities by size, please wait ...\n","PhenoGraph completed in 130.5205729007721 seconds\n","Finding 30 nearest neighbors using minkowski metric and 'auto' algorithm\n","Neighbors computed in 27.621578216552734 seconds\n","Jaccard graph constructed in 17.766773462295532 seconds\n","Wrote graph to binary file in 0.8931858539581299 seconds\n","Running Louvain modularity optimization\n","After 1 runs, maximum modularity is Q = 0.751925\n","After 2 runs, maximum modularity is Q = 0.754312\n","After 5 runs, maximum modularity is Q = 0.757418\n","Louvain completed 25 runs in 67.26227569580078 seconds\n","Sorting communities by size, please wait ...\n","PhenoGraph completed in 113.81405401229858 seconds\n"]}],"source":["import pandas as pd\n","\n","###### process protein modality\n","\n","data1 = pd.read_csv(\"/content/drive/MyDrive/spatial_cluster/processed_codex_murine/features_and_metadata.csv\")\n","dataa = data1.iloc[:, 3:34] # drop non-protein columns\n","dataa = dataa.drop('nucl',axis = 1)\n","\n","## change to scaled version\n","from scipy.stats import zscore\n","dataa = dataa.apply(zscore)\n","dataa = dataa\n","\n","####### process imaging modality\n","\n","datab_full = np.load('/content/drive/MyDrive/MaxFuse_related/MUSE_related/inception_imag_feature_spleen53500.npy')\n","## change img feature sequence to real\n","llreal = np.load('/content/drive/MyDrive/MaxFuse_related/MUSE_related/realidx_full.npy')\n","datab_full = datab_full[llreal,:]\n","\n","###### this is pca on feature mod and image mod\n","\n","latent_dim = 20\n","view_a_feature = PCA(n_components=latent_dim).fit_transform(dataa)\n","### this is pca on img mod\n","latent_dim = 100\n","view_b_feature = PCA(n_components=latent_dim).fit_transform(datab_full)\n","### then default clustering\n","view_a_label, _, _ = phenograph.cluster(view_a_feature)\n","view_b_label, _, _ = phenograph.cluster(view_b_feature)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# muse fit function but close multiple cpu option other wise exceed server capability.\n","# change: n_jobs = 16 --> n_jobs = 2 to avoid shutdown in cases\n","\n","import numpy as np\n","from muse_sc.muse_architecture import structured_embedding\n","from scipy.spatial.distance import pdist\n","import phenograph\n","import tensorflow.compat.v1 as tf\n","\n","tf.disable_v2_behavior()\n","\n","def muse_fit_predict(data_x,\n","                     data_y,\n","                     label_x,\n","                     label_y,\n","                     latent_dim=100,\n","                     n_epochs=500,\n","                     lambda_regul=5,\n","                     lambda_super=5):\n","    \"\"\"\n","        MUSE model fitting and predicting:\n","          This function is used to train the MUSE model on multi-modality data\n","\n","        Parameters:\n","          data_x:       input for transcript modality; matrix of  n * p, where n = number of cells, p = number of genes.\n","          data_y:       input for morphological modality; matrix of n * q, where n = number of cells, q is the feature dimension.\n","          label_x:      initial reference cluster label for transcriptional modality.\n","          label_y:      inital reference cluster label for morphological modality.\n","          latent_dim:   feature dimension of joint latent representation.\n","          n_epochs:     maximal epoch used in training.\n","          lambda_regul: weight for regularization term in the loss function.\n","          lambda_super: weight for supervised learning loss in the loss function.\n","\n","        Output:\n","          latent:       joint latent representation learned by MUSE.\n","          reconstruct_x:reconstructed feature matrix corresponding to input data_x.\n","          reconstruct_y:reconstructed feature matrix corresponding to input data_y.\n","          latent_x:     modality-specific latent representation corresponding to data_x.\n","          latent_y:     modality-specific latent representation corresponding to data_y.\n","\n","        Feng Bao @ Altschuler & Wu Lab @ UCSF 2022.\n","        Software provided as is under MIT License.\n","    \"\"\"\n","\n","    \"\"\" initial parameter setting \"\"\"\n","    # parameter setting for neural network\n","    n_hidden = 128  # number of hidden node in neural network\n","    learn_rate = 1e-4  # learning rate in the optimization\n","    batch_size = 64  # number of cells in the training batch\n","    n_epochs_init = 200  # number of training epoch in model initialization\n","    print_epochs = 50  # epoch interval to display the current training loss\n","    cluster_update_epoch = 200  # epoch interval to update modality-specific clusters\n","\n","    # read data-specific parameters from inputs\n","    feature_dim_x = data_x.shape[1]\n","    feature_dim_y = data_y.shape[1]\n","    n_sample = data_x.shape[0]\n","\n","    # GPU configuration\n","    # config = tf.ConfigProto()\n","    # config.gpu_options.allow_growth = True\n","\n","    \"\"\" construct computation graph using TensorFlow \"\"\"\n","    tf.reset_default_graph()\n","\n","    # raw data from two modalities\n","    x = tf.placeholder(tf.float32, shape=[None, feature_dim_x], name='input_x')\n","    y = tf.placeholder(tf.float32, shape=[None, feature_dim_y], name='input_y')\n","\n","    # labels inputted for references\n","    ref_label_x = tf.placeholder(tf.float32, shape=[None], name='ref_label_x')\n","    ref_label_y = tf.placeholder(tf.float32, shape=[None], name='ref_label_y')\n","\n","    # hyperparameter in triplet loss\n","    triplet_lambda = tf.placeholder(tf.float32, name='triplet_lambda')\n","    triplet_margin = tf.placeholder(tf.float32, name='triplet_margin')\n","\n","    # network architecture\n","    z, x_hat, y_hat, encode_x, encode_y, loss, \\\n","    reconstruction_error, weight_penalty, \\\n","    trip_loss_x, trip_loss_y = structured_embedding(x,\n","                                                    y,\n","                                                    ref_label_x,\n","                                                    ref_label_y,\n","                                                    latent_dim,\n","                                                    triplet_margin,\n","                                                    n_hidden,\n","                                                    lambda_regul,\n","                                                    triplet_lambda)\n","    # optimization operator\n","    train_op = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n","    print('++++++++++ MUSE for multi-modality single-cell analysis ++++++++++')\n","    \"\"\" MUSE optimization \"\"\"\n","    total_batch = int(n_sample / batch_size)\n","\n","    with tf.Session() as sess:\n","\n","        \"\"\" initialization of autoencoder architecture for MUSE \"\"\"\n","        print('MUSE initialization')\n","        # global parameter initialization\n","        sess.run(tf.global_variables_initializer(), feed_dict={triplet_lambda: 0,\n","                                                               triplet_margin: 0})\n","\n","        for epoch in range(n_epochs_init):\n","            # randomly permute samples\n","            random_idx = np.random.permutation(n_sample)\n","            data_train_x = data_x[random_idx, :]\n","            data_train_y = data_y[random_idx, :]\n","\n","            for i in range(total_batch):\n","                # input data batches\n","                offset = (i * batch_size) % (n_sample)\n","                batch_x_input = data_train_x[offset:(offset + batch_size), :]\n","                batch_y_input = data_train_y[offset:(offset + batch_size), :]\n","\n","                # initialize parameters without self-supervised loss (triplet_lambda=0)\n","                sess.run(train_op,\n","                         feed_dict={x: batch_x_input,\n","                                    y: batch_y_input,\n","                                    ref_label_x: np.zeros(batch_x_input.shape[0]),\n","                                    ref_label_y: np.zeros(batch_y_input.shape[0]),\n","                                    triplet_lambda: 0,\n","                                    triplet_margin: 0})\n","\n","            # calculate and print loss terms for current epoch\n","            if epoch % print_epochs == 0:\n","                L_total, L_reconstruction, L_weight = \\\n","                    sess.run((loss, reconstruction_error, weight_penalty),\n","                             feed_dict={x: data_train_x,\n","                                        y: data_train_y,\n","                                        ref_label_x: np.zeros(data_train_x.shape[0]),  # no use as triplet_lambda=0\n","                                        ref_label_y: np.zeros(data_train_y.shape[0]),  # no use as triplet_lambda=0\n","                                        triplet_lambda: 0,\n","                                        triplet_margin: 0})\n","\n","                print(\n","                    \"epoch: %d, \\t total loss: %03.5f,\\t reconstruction loss: %03.5f,\\t sparse penalty: %03.5f\"\n","                    % (epoch, L_total, L_reconstruction, L_weight))\n","\n","        # estimate the margin for the triplet loss\n","        latent, reconstruct_x, reconstruct_y = \\\n","            sess.run((z, x_hat, y_hat),\n","                     feed_dict={x: data_x,\n","                                y: data_y,\n","                                ref_label_x: np.zeros(data_x.shape[0]),\n","                                ref_label_y: np.zeros(data_y.shape[0]),\n","                                triplet_lambda: 0,\n","                                triplet_margin: 0})\n","        latent_pd_matrix = pdist(latent, 'euclidean')\n","        latent_pd_sort = np.sort(latent_pd_matrix)\n","        select_top_n = np.int(latent_pd_sort.size * 0.2)\n","        margin_estimate = np.median(latent_pd_sort[-select_top_n:]) - np.median(latent_pd_sort[:select_top_n])\n","\n","        # refine MUSE parameters with reference labels and triplet losses\n","        for epoch in range(n_epochs_init):\n","            # randomly permute samples\n","            random_idx = np.random.permutation(n_sample)\n","            data_train_x = data_x[random_idx, :]\n","            data_train_y = data_y[random_idx, :]\n","            label_train_x = label_x[random_idx]\n","            label_train_y = label_y[random_idx]\n","\n","            for i in range(total_batch):\n","                # data batches\n","                offset = (i * batch_size) % (n_sample)\n","                batch_x_input = data_train_x[offset:(offset + batch_size), :]\n","                batch_y_input = data_train_y[offset:(offset + batch_size), :]\n","                label_x_input = label_train_x[offset:(offset + batch_size)]\n","                label_y_input = label_train_y[offset:(offset + batch_size)]\n","\n","                # refine parameters\n","                sess.run(train_op,\n","                         feed_dict={x: batch_x_input,\n","                                    y: batch_y_input,\n","                                    ref_label_x: label_x_input,\n","                                    ref_label_y: label_y_input,\n","                                    triplet_lambda: lambda_super,\n","                                    triplet_margin: margin_estimate})\n","\n","            # calculate loss on all input data for current epoch\n","            if epoch % print_epochs == 0:\n","                L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y = \\\n","                    sess.run((loss, reconstruction_error, weight_penalty, trip_loss_x, trip_loss_y),\n","                             feed_dict={x: data_train_x,\n","                                        y: data_train_y,\n","                                        ref_label_x: label_train_x,\n","                                        ref_label_y: label_train_y,\n","                                        triplet_lambda: lambda_super,\n","                                        triplet_margin: margin_estimate})\n","\n","                print(\n","                    \"epoch: %d, \\t total loss: %03.5f,\\t reconstruction loss: %03.5f,\\t sparse penalty: %03.5f,\\t x triplet: %03.5f,\\t y triplet: %03.5f\"\n","                    % (epoch, L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y))\n","\n","        # update cluster labels based modality-specific latents\n","        latent_x, latent_y = \\\n","            sess.run((encode_x, encode_y),\n","                     feed_dict={x: data_x,\n","                                y: data_y,\n","                                ref_label_x: label_x,\n","                                ref_label_y: label_y,\n","                                triplet_lambda: lambda_super,\n","                                triplet_margin: margin_estimate})\n","\n","        # update cluster labels using PhenoGraph\n","        label_x_update, _, _ = phenograph.cluster(latent_x, n_jobs= 2)\n","        label_y_update, _, _ = phenograph.cluster(latent_y, n_jobs= 2)\n","        print('Finish initialization of MUSE')\n","\n","        ''' Training of MUSE '''\n","        for epoch in range(n_epochs):\n","            # randomly permute samples\n","            random_idx = np.random.permutation(n_sample)\n","            data_train_x = data_x[random_idx, :]\n","            data_train_y = data_y[random_idx, :]\n","            label_train_x = label_x_update[random_idx]\n","            label_train_y = label_y_update[random_idx]\n","\n","            # loop over all batches\n","            for i in range(total_batch):\n","                # batch data\n","                offset = (i * batch_size) % (n_sample)\n","                batch_x_input = data_train_x[offset:(offset + batch_size), :]\n","                batch_y_input = data_train_y[offset:(offset + batch_size), :]\n","                batch_label_x_input = label_train_x[offset:(offset + batch_size)]\n","                batch_label_y_input = label_train_y[offset:(offset + batch_size)]\n","\n","                sess.run(train_op,\n","                         feed_dict={x: batch_x_input,\n","                                    y: batch_y_input,\n","                                    ref_label_x: batch_label_x_input,\n","                                    ref_label_y: batch_label_y_input,\n","                                    triplet_lambda: lambda_super,\n","                                    triplet_margin: margin_estimate})\n","\n","            # calculate and print losses on whole training dataset\n","            if epoch % print_epochs == 0:\n","                L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y = \\\n","                    sess.run((loss, reconstruction_error, weight_penalty, trip_loss_x, trip_loss_y),\n","                             feed_dict={x: data_train_x,\n","                                        y: data_train_y,\n","                                        ref_label_x: label_train_x,\n","                                        ref_label_y: label_train_y,\n","                                        triplet_lambda: lambda_super,\n","                                        triplet_margin: margin_estimate})\n","                # print cost every epoch\n","                print(\n","                    \"epoch: %d, \\t total loss: %03.5f,\\t reconstruction loss: %03.5f,\\t sparse penalty: %03.5f,\\t x triplet loss: %03.5f,\\t y triplet loss: %03.5f\"\n","                    % (epoch, L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y))\n","\n","            # update cluster labels based on new modality-specific latent representations\n","            if epoch % cluster_update_epoch == 0:\n","                latent_x, latent_y = \\\n","                    sess.run((encode_x, encode_y),\n","                             feed_dict={x: data_x,\n","                                        y: data_y,\n","                                        ref_label_x: label_x,\n","                                        ref_label_y: label_y,\n","                                        triplet_lambda: lambda_super,\n","                                        triplet_margin: margin_estimate})\n","\n","                # use PhenoGraph to obtain cluster label\n","                label_x_update, _, _ = phenograph.cluster(latent_x, n_jobs= 2)\n","                label_y_update, _, _ = phenograph.cluster(latent_y, n_jobs= 2)\n","\n","        \"\"\" MUSE output \"\"\"\n","        latent, reconstruct_x, reconstruct_y, latent_x, latent_y = \\\n","            sess.run((z, x_hat, y_hat, encode_x, encode_y),\n","                     feed_dict={x: data_x,\n","                                y: data_y,\n","                                ref_label_x: label_x,  # no effects to representations\n","                                ref_label_y: label_y,  # no effects to representations\n","                                triplet_lambda: lambda_super,\n","                                triplet_margin: margin_estimate})\n","\n","        print('++++++++++ MUSE completed ++++++++++')\n","\n","    return latent, reconstruct_x, reconstruct_y, latent_x, latent_y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"elapsed":36885,"status":"error","timestamp":1686260668714,"user":{"displayName":"Bokai Zhu","userId":"09132585489018838978"},"user_tz":420},"id":"iLvI0_6L2rAZ","outputId":"94d4181e-06a6-429d-e07d-b2f085bf192d"},"outputs":[],"source":["## loop muse with batching behavior other wise memory exceed in cases\n","\n","################################# loop batch version, random sample\n","for i in range(5):\n","\n","  np.random.seed(i)\n","  indices = np.random.choice(datab_full.shape[0], 10000, replace=False)\n","  dataa_sub = dataa.iloc[indices,:]\n","  datab_sub = datab_full[indices,:]\n","  view_a_label_sub = view_a_label[indices]\n","  view_b_label_sub = view_b_label[indices]\n","\n","  muse_feature, reconstruct_x, reconstruct_y, \\\n","    latent_x, latent_y = muse_fit_predict(dataa_sub.to_numpy(),\n","                                          datab_sub,\n","                                          view_a_label_sub,\n","                                          view_b_label_sub,\n","                                          latent_dim=30,\n","                                          n_epochs=500,\n","                                          lambda_regul=5,\n","                                          lambda_super=5)\n","  name = 'batch_' + str(i) + '.npy'\n","  np.save('../data/muse_murine_embedding_' + name, muse_feature)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
